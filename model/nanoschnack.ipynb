{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce047449aa6ea1b",
   "metadata": {},
   "source": [
    "# NanoSchnack Model\n",
    "\n",
    "## Setup\n",
    "\n",
    "- Install dependencies.\n",
    "- Verify that MPS is available (for Apple Silicon GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe08928ac36354a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T17:27:20.105793Z",
     "start_time": "2025-12-20T17:27:20.072300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pickletools import optimize\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.backends.mps.is_available()\n",
    "torch.backends.mps.is_built()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291720d5f320a7a5",
   "metadata": {},
   "source": [
    "## Trying out MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "313e2a5486c901ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T19:40:28.863562Z",
     "start_time": "2025-12-20T19:40:28.849965Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f746ce3bad99a",
   "metadata": {},
   "source": [
    "## Loading a tokenizer with Hugging Face's tokenizer library\n",
    "\n",
    "- Compare: https://github.com/huggingface/tokenizers\n",
    "- Tiktokenizer: https://tiktokenizer.vercel.app/?model=gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db31e04be138c071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T19:40:37.830688Z",
     "start_time": "2025-12-20T19:40:37.649390Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer_path = hf_hub_download(repo_id=\"openai-community/gpt2\", filename=\"tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a676960136ed2",
   "metadata": {},
   "source": [
    "### Testing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "160288461bd092ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T19:41:54.042655Z",
     "start_time": "2025-12-20T19:41:53.943081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 2159, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(tokenizer.encode(\"Hello, World!\").ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dbfcc99775200",
   "metadata": {},
   "source": [
    "## Instantiating the NanoSchnack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71dce028f07aaaf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T19:55:41.791580Z",
     "start_time": "2025-12-20T19:55:39.600918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.98109245300293, Loss per bit: 0.247537\n",
      "Epoch 2, Loss: 10.98193645477295, Loss per bit: 0.247556\n",
      "Epoch 3, Loss: 11.02159309387207, Loss per bit: 0.248450\n",
      "Epoch 4, Loss: 11.055585861206055, Loss per bit: 0.249216\n",
      "Epoch 5, Loss: 11.043811798095703, Loss per bit: 0.248951\n",
      "Epoch 6, Loss: 11.025858879089355, Loss per bit: 0.248546\n",
      "Epoch 7, Loss: 11.04798412322998, Loss per bit: 0.249045\n",
      "Epoch 8, Loss: 10.961246490478516, Loss per bit: 0.247090\n",
      "Epoch 9, Loss: 10.976751327514648, Loss per bit: 0.247439\n",
      "Epoch 10, Loss: 10.938393592834473, Loss per bit: 0.246574\n"
     ]
    }
   ],
   "source": [
    "from gpt import GPT\n",
    "\n",
    "model = GPT(vocab_size=tokenizer.get_vocab_size()).to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dffa18e4d2fb4",
   "metadata": {},
   "source": [
    "## Load the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ea7406035cdae00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T20:45:16.345960Z",
     "start_time": "2025-12-20T20:45:15.959952Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load dataset in streaming mode (does not load everything into memory at once)\n",
    "# Note(sttts): I am using https://huggingface.co/datasets/pdelobelle/fineweb-german-edu-mt.\n",
    "raw_ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"train\": \"../data/*.parquet\"},\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# Shuffle the dataset with a buffer for approximate shuffling\n",
    "shuffled = raw_ds.shuffle(buffer_size=10_000, seed=42) # lazy shuffle (approximate) with a buffer\n",
    "\n",
    "# Enable truncation and padding\n",
    "tokenizer.enable_truncation(max_length=128)\n",
    "tokenizer.enable_padding(length=128, pad_id=0, pad_token=\"[PAD]\")\n",
    "\n",
    "# Wrap Hugging Face tokenizer for batch processing\n",
    "def tokenizer_batch(batch):\n",
    "    token_batch = tokenizer.encode_batch(batch[\"result\"])\n",
    "    return {\n",
    "        \"input_ids\": [e.ids for e in token_batch],\n",
    "        \"attention_mask\": [e.attention_mask for e in token_batch],\n",
    "    }\n",
    "dataset = shuffled.map(tokenizer_batch, batched=True)\n",
    "dataset = dataset.with_format(type=\"torch\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484ece969be72dd",
   "metadata": {},
   "source": [
    "## Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9416b74888ed605b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T21:10:44.550299Z",
     "start_time": "2025-12-20T21:07:30.174949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.674805641174316, Loss per bit: 0.127922\n",
      "Epoch 2, Loss: 5.538754940032959, Loss per bit: 0.124855\n",
      "Epoch 3, Loss: 5.391127109527588, Loss per bit: 0.121527\n",
      "Epoch 4, Loss: 5.268251419067383, Loss per bit: 0.118758\n",
      "Epoch 5, Loss: 5.146932601928711, Loss per bit: 0.116023\n",
      "Epoch 6, Loss: 5.035008907318115, Loss per bit: 0.113500\n",
      "Epoch 7, Loss: 4.936888217926025, Loss per bit: 0.111288\n",
      "Epoch 8, Loss: 4.842888832092285, Loss per bit: 0.109169\n",
      "Epoch 9, Loss: 4.747250556945801, Loss per bit: 0.107013\n",
      "Epoch 10, Loss: 4.656844139099121, Loss per bit: 0.104975\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10_000)\n",
    "lossFn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "steps_per_epoch = 10\n",
    "for epoch in range(10):\n",
    "    for step, batch in enumerate(loader):\n",
    "        if step >= steps_per_epoch:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        # Next-token prediction\n",
    "        inputs = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = lossFn(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    lossPerBit = loss.item() / (16 * 4) / torch.log(torch.tensor(2.0))\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Loss per bit: {lossPerBit:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
