{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce047449aa6ea1b",
   "metadata": {},
   "source": [
    "# NanoSchnack Model\n",
    "\n",
    "## Setup\n",
    "\n",
    "- Install dependencies.\n",
    "- Verify that MPS is available (for Apple Silicon GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe08928ac36354a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T07:22:38.516800Z",
     "start_time": "2025-12-21T07:22:37.704597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pickletools import optimize\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.backends.mps.is_available()\n",
    "torch.backends.mps.is_built()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291720d5f320a7a5",
   "metadata": {},
   "source": [
    "## Trying out MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313e2a5486c901ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T07:22:38.521318Z",
     "start_time": "2025-12-21T07:22:38.517991Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f746ce3bad99a",
   "metadata": {},
   "source": [
    "## Loading a tokenizer with Hugging Face's tokenizer library\n",
    "\n",
    "- Compare: https://github.com/huggingface/tokenizers\n",
    "- Tiktokenizer: https://tiktokenizer.vercel.app/?model=gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db31e04be138c071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T07:22:41.900811Z",
     "start_time": "2025-12-21T07:22:38.521634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sts/src/nanoschnack/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer_path = hf_hub_download(repo_id=\"openai-community/gpt2\", filename=\"tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a676960136ed2",
   "metadata": {},
   "source": [
    "### Testing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160288461bd092ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T07:22:42.007985Z",
     "start_time": "2025-12-21T07:22:41.953911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 2159, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(tokenizer.encode(\"Hello, World!\").ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dbfcc99775200",
   "metadata": {},
   "source": [
    "## Instantiating the NanoSchnack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71dce028f07aaaf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T07:22:42.375213Z",
     "start_time": "2025-12-21T07:22:42.009430Z"
    }
   },
   "outputs": [],
   "source": [
    "from gpt import GPT\n",
    "\n",
    "# add special tokens\n",
    "tokenizer.add_special_tokens([\"[PAD]\"])\n",
    "pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "\n",
    "context_len = 256\n",
    "model = GPT(vocab_size=tokenizer.get_vocab_size()).to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dffa18e4d2fb4",
   "metadata": {},
   "source": [
    "## Load the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea7406035cdae00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T07:22:43.432073Z",
     "start_time": "2025-12-21T07:22:42.375591Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load dataset in streaming mode (does not load everything into memory at once)\n",
    "# Note(sttts): I am using https://huggingface.co/datasets/pdelobelle/fineweb-german-edu-mt.\n",
    "raw_ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"train\": \"../data/*.parquet\"},\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# Shuffle the dataset with a buffer for approximate shuffling\n",
    "shuffled = raw_ds.shuffle(buffer_size=10_000, seed=42) # lazy shuffle (approximate) with a buffer\n",
    "\n",
    "# do or not do chunking of the input text, instead of truncating.\n",
    "if False:\n",
    "    max_len = context_len\n",
    "    stride = context_len/4  # overlap; set to 0 for no overlap\n",
    "\n",
    "    tokenizer.disable_truncation()\n",
    "    tokenizer.disable_padding()\n",
    "\n",
    "    # Split long sequences into fixed windows, optionally with overlap.\n",
    "    def chunk_ids(ids, max_len, stride):\n",
    "        if len(ids) == 0:\n",
    "            return []\n",
    "        step = max_len - stride\n",
    "        chunks = []\n",
    "        for start in range(0, len(ids), step):\n",
    "            chunk = ids[start:start + max_len]\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            if len(chunk) < max_len:\n",
    "                chunk = chunk + [pad_id] * (max_len - len(chunk))\n",
    "            chunks.append(chunk)\n",
    "            if start + max_len >= len(ids):\n",
    "                break\n",
    "        return chunks\n",
    "\n",
    "    def tokenizer_batch(batch):\n",
    "        input_ids = []\n",
    "        attention_mask = [] # marks real tokens (1) vs padding (0)\n",
    "        for text in batch[\"result\"]:\n",
    "            ids = tokenizer.encode(text).ids\n",
    "            for chunk in chunk_ids(ids, max_len=max_len,\n",
    "                                   stride=stride):\n",
    "                input_ids.append(chunk)\n",
    "                attention_mask.append([1 if t != pad_id else 0 for t\n",
    "                                       in chunk])\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "else:\n",
    "    # Enable truncation and padding\n",
    "    tokenizer.enable_truncation(max_length=context_len)\n",
    "    tokenizer.enable_padding(length=context_len, pad_id=pad_id, pad_token=\"[PAD]\")\n",
    "\n",
    "    # Wrap Hugging Face tokenizer for batch processing\n",
    "    def tokenizer_batch(batch):\n",
    "        token_batch = tokenizer.encode_batch(batch[\"result\"])\n",
    "        return {\n",
    "            \"input_ids\": [e.ids for e in token_batch],\n",
    "            \"attention_mask\": [e.attention_mask for e in token_batch], # marks real tokens (1) vs padding (0)\n",
    "        }\n",
    "\n",
    "# Shuffle deterministically (only way for streaming datasets)\n",
    "dataset = shuffled.map(tokenizer_batch, batched=True)\n",
    "\n",
    "# Set the dataset format to PyTorch tensors\n",
    "dataset = dataset.with_format(type=\"torch\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "batch_size = 32\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484ece969be72dd",
   "metadata": {},
   "source": [
    "## Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416b74888ed605b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-21T07:22:43.442758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 7), Loss: 10.1352, Samples/s: 20.2\n",
      "Epoch 1 (Step 14), Loss: 9.4491, Samples/s: 20.0\n",
      "Epoch 1 (Step 21), Loss: 9.0201, Samples/s: 18.7\n",
      "Epoch 1 (Step 27), Loss: 8.5242, Samples/s: 18.1\n",
      "Epoch 1 (Step 33), Loss: 8.0164, Samples/s: 17.7\n",
      "loss (last hour, min 7.8498 max 10.9798)\n",
      "   10.98  ┤\n",
      "   10.70  ┼──╮\n",
      "   10.41  ┤  ╰───╮\n",
      "   10.13  ┤      ╰──╮\n",
      "    9.84  ┤         ╰────╮\n",
      "    9.56  ┤              ╰───╮\n",
      "    9.27  ┤                  ╰──────╮\n",
      "    8.99  ┤                         ╰─────────╮\n",
      "    8.70  ┤                                   ╰──────╮\n",
      "    8.42  ┤                                          ╰──╮ ╭─╮\n",
      "    8.13  ┤                                             ╰─╯ ╰──────╮\n",
      "    7.85  ┤                                                        ╰──\n",
      "           08:22:45                 08:23:15                  08:23:45\n"
     ]
    }
   ],
   "source": [
    "from plot import ascii_loss_plot\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10_000)\n",
    "lossFn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 1 # epochs between 1 and 3 are usually sufficient for good results, rather 1 than 3.\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "last_plot_time = start_time\n",
    "samples_since_log = 0\n",
    "loss_history = deque()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(loader):\n",
    "        # Get the input IDs and attention mask, and move them to the GPU\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Next-token prediction\n",
    "        inputs = input_ids[:, :-1] # everything from the first token except the last\n",
    "        targets = input_ids[:, 1:] # everything from the second token onward\n",
    "\n",
    "        # Clear accumulated gradients from the previous step (which torch does automatically otherwise)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(inputs, attention_mask=attention_mask[:, :-1])\n",
    "\n",
    "        # Compute (average) loss of the predicted next tokens and apply backpropagation.\n",
    "        # reshape to (batch_size * seq_len, vocab_size) and (batch_size * seq_len)\n",
    "        loss = lossFn(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights, then advance the learning-rate schedule.\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Record loss for logging and plotting\n",
    "        now = time.time()\n",
    "        loss_history.append((now, loss.item()))\n",
    "        while loss_history and (now - loss_history[0][0]) > 3600:\n",
    "            loss_history.popleft()\n",
    "\n",
    "        # Every 10 seconds, log progress\n",
    "        samples_since_log += input_ids.size(0)\n",
    "        if now - last_log_time >= 10:\n",
    "            elapsed = now - last_log_time\n",
    "            samples_per_sec = samples_since_log / elapsed if elapsed > 0 else 0.0\n",
    "            print(f\"Epoch {epoch+1}, Step {step+1}, Loss: {loss.item():.4f}, Samples/s: {samples_per_sec:.1f}\")\n",
    "            last_log_time = now\n",
    "            samples_since_log = 0\n",
    "\n",
    "        # Every minute (or every 10 minutes after 10 minutes), plot loss history\n",
    "        plot_interval = 60 if (now - start_time) < 600 else 600\n",
    "        if now - last_plot_time >= plot_interval:\n",
    "            print(ascii_loss_plot(list(loss_history)))\n",
    "            last_plot_time = now\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
