{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce047449aa6ea1b",
   "metadata": {},
   "source": [
    "# NanoSchnack Model\n",
    "\n",
    "## Setup\n",
    "\n",
    "- Install dependencies.\n",
    "- Verify that MPS is available (for Apple Silicon GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe08928ac36354a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:00:23.419351Z",
     "start_time": "2025-12-21T08:00:22.409179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contextlib\n",
    "import torch\n",
    "from device import device_info, pick_device, print_device_info\n",
    "\n",
    "device = pick_device()\n",
    "info = device_info(device)\n",
    "print_device_info(info)\n",
    "# Report SDPA kernel availability for attention debugging.\n",
    "print(\"Performance:\")\n",
    "print(f\"  Flash SDP enabled: {torch.backends.cuda.flash_sdp_enabled()}\")\n",
    "print(f\"  Mem-efficient SDP enabled: {torch.backends.cuda.mem_efficient_sdp_enabled()}\")\n",
    "print(f\"  Math SDP enabled: {torch.backends.cuda.math_sdp_enabled()}\")\n",
    "print(\"  SDPA kernel selection: set TORCH_LOGS=attention\")\n",
    "\n",
    "# Switch to TF32 for 8x speedup on supported hardware, and good enough for LLM training.\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f746ce3bad99a",
   "metadata": {},
   "source": [
    "## Loading a tokenizer with Hugging Face's tokenizer library\n",
    "\n",
    "- Compare: https://github.com/huggingface/tokenizers\n",
    "- Tiktokenizer: https://tiktokenizer.vercel.app/?model=gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db31e04be138c071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:00:23.702221Z",
     "start_time": "2025-12-21T08:00:23.423961Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sts/src/nanoschnack/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import load_tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dbfcc99775200",
   "metadata": {},
   "source": [
    "## Instantiating the NanoSchnack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71dce028f07aaaf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:00:24.153419Z",
     "start_time": "2025-12-21T08:00:23.767280Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from gpt import GPT\n",
    "from autotune import find_max_batch_size\n",
    "import config\n",
    "\n",
    "# Resolve model paths so relative data/checkpoint locations are stable.\n",
    "try:\n",
    "    from model import setup_paths\n",
    "except ModuleNotFoundError:\n",
    "    from __init__ import setup_paths\n",
    "model_dir, data_dir, checkpoint_dir = setup_paths()\n",
    "\n",
    "# Pull model sizes from the most recent checkpoint if present.\n",
    "import torch\n",
    "checkpoint_path = checkpoint_dir / \"latest.pt\"\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint_state = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    if isinstance(checkpoint_state, dict) and \"config\" in checkpoint_state:\n",
    "        ckpt_config = checkpoint_state[\"config\"]\n",
    "        config.CONTEXT_LEN = ckpt_config.get(\"CONTEXT_LEN\", config.CONTEXT_LEN)\n",
    "        config.EMBED_SIZE = ckpt_config.get(\"EMBED_SIZE\", config.EMBED_SIZE)\n",
    "        config.NUM_LAYERS = ckpt_config.get(\"NUM_LAYERS\", config.NUM_LAYERS)\n",
    "        config.NUM_HEADS = ckpt_config.get(\"NUM_HEADS\", config.NUM_HEADS)\n",
    "        config.HIDDEN_SIZE = ckpt_config.get(\"HIDDEN_SIZE\", config.HIDDEN_SIZE)\n",
    "\n",
    "context_len = config.CONTEXT_LEN\n",
    "embed_size = config.EMBED_SIZE\n",
    "num_layers = config.NUM_LAYERS\n",
    "num_heads = config.NUM_HEADS\n",
    "hidden_size = config.HIDDEN_SIZE\n",
    "\n",
    "# add special tokens\n",
    "tokenizer.add_special_tokens([\"[PAD]\"])\n",
    "pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    embed_size=embed_size,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    hidden_size=hidden_size,\n",
    "    context_len=context_len,\n",
    ").to(device).train()\n",
    "\n",
    "# Now with the tokenizer derive training parameters like batch size.\n",
    "tuned_batch_size = find_max_batch_size(\n",
    "    model,\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    seq_len=context_len,\n",
    "    device=device,\n",
    "    start=config.BATCH_SIZE,\n",
    ")\n",
    "if tuned_batch_size:\n",
    "    config.BATCH_SIZE = tuned_batch_size\n",
    "\n",
    "# Compile the model for faster training.\n",
    "if device.type == \"cuda\":\n",
    "    print(\"Compiling the model for faster training...\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "param_count, quantization = config.model_info(model)\n",
    "config.print_training_hyperparams(param_count=param_count, quantization=quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dffa18e4d2fb4",
   "metadata": {},
   "source": [
    "## Load the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea7406035cdae00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:00:25.276715Z",
     "start_time": "2025-12-21T08:00:24.153843Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.utils.logging import enable_progress_bar, set_verbosity_warning\n",
    "from loader import (\n",
    "    TokenEstimator,\n",
    "    build_interleaved_dataset,\n",
    "    build_packed_dataset,\n",
    "    load_dataset_from_spec,\n",
    "    parse_dataset_specs,\n",
    "    resolve_resume_plan,\n",
    "    resolve_total_rows,\n",
    "    dataset_label,\n",
    ")\n",
    "from resume import build_resume_state, normalize_resume_rows\n",
    "import math\n",
    "\n",
    "# Download shards on demand and shuffle within each dataset.\n",
    "set_verbosity_warning()\n",
    "enable_progress_bar()\n",
    "\n",
    "# Cache dataset specs for reuse across steps.\n",
    "dataset_specs = parse_dataset_specs(config.DATASET_SPECS)\n",
    "estimated_total_tokens = 0\n",
    "\n",
    "print(\"Estimating tokens from dataset samples...\", flush=True)\n",
    "for dataset_index, spec in enumerate(dataset_specs):\n",
    "    raw_dataset = load_dataset_from_spec(\n",
    "        spec,\n",
    "        cache_dir=data_dir,\n",
    "        streaming=True,\n",
    "    )\n",
    "    total_rows = resolve_total_rows(raw_dataset, spec)\n",
    "    if total_rows is None:\n",
    "        raise ValueError(\"Dataset split metadata missing num_examples for token estimate.\")\n",
    "    token_estimator = TokenEstimator(tokenizer, text_key=spec[\"text_key\"])\n",
    "    avg_tokens, est_total_tokens = token_estimator.estimate_streaming(raw_dataset, total_rows)\n",
    "    estimated_total_tokens += est_total_tokens\n",
    "    print(\n",
    "        f\"Dataset {dataset_index + 1}/{len(dataset_specs)} \"\n",
    "        f\"({dataset_label(spec)}): avg_tokens={avg_tokens:.1f}, \"\n",
    "        f\"est_tokens={est_total_tokens}\"\n",
    "    )\n",
    "\n",
    "# Resolve model size for token budgeting.\n",
    "param_count, _ = config.model_info(model)\n",
    "\n",
    "# Derive the token cap and epoch count from the configured max-training factor.\n",
    "max_tokens = int(param_count * config.MAX_TRAINING_FACTOR) if config.MAX_TRAINING_FACTOR > 0 else 0\n",
    "target_tokens = max_tokens or estimated_total_tokens\n",
    "target_epochs = 1\n",
    "if max_tokens and estimated_total_tokens > 0:\n",
    "    target_epochs = max(1, math.ceil(target_tokens / estimated_total_tokens))\n",
    "tokens_per_step = config.BATCH_SIZE * (config.CONTEXT_LEN - 1)\n",
    "dataset_steps = math.ceil(estimated_total_tokens / tokens_per_step)\n",
    "print(\n",
    "    f\"Dataset estimate: steps={dataset_steps:,} tokens={estimated_total_tokens:,} \"\n",
    "    f\"tokens_per_step={tokens_per_step:,}\",\n",
    "    flush=True,\n",
    ")\n",
    "print(\n",
    "    f\"Target:          epochs={target_epochs:,} target_tokens={target_tokens:,} \"\n",
    "    f\"(factor {config.MAX_TRAINING_FACTOR} of model size {param_count:,})\",\n",
    "    flush=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13a24932565279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Progress and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eb73238108655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_completion(points):\n",
    "    # Render the loss plot first so completion failures don't block logs.\n",
    "    chart = ascii_loss_plot(points)\n",
    "\n",
    "    # Append the configured completion snapshot.\n",
    "    was_training = model.training\n",
    "    if was_training:\n",
    "        model.eval()\n",
    "    try:\n",
    "        reply_parts = []\n",
    "        for token in generate_reply_stream(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                config.PLOT_COMPLETION_PROMPT,\n",
    "                context_len=config.CONTEXT_LEN,\n",
    "                max_new_tokens=config.PLOT_COMPLETION_TOKENS,\n",
    "                temperature=config.TEMPERATURE,\n",
    "                top_k=config.TOP_K,\n",
    "                device=device,\n",
    "        ):\n",
    "            reply_parts.append(token)\n",
    "        completion = \"\".join(reply_parts)\n",
    "    except Exception as exc:\n",
    "        completion = f\" [generation failed: {exc}]\"\n",
    "    finally:\n",
    "        if was_training:\n",
    "            model.train()\n",
    "    return (\n",
    "        f\"{chart}\\n\\ncompletion ({config.PLOT_COMPLETION_TOKENS} tokens)\\n\"\n",
    "        f\"{config.PLOT_COMPLETION_PROMPT}{completion}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430e00e19d9c480",
   "metadata": {},
   "source": [
    "## Load the previous Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f447f7cbb26e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import ascii_loss_plot\n",
    "from chat import generate_reply_stream\n",
    "from progress import ProgressLogger\n",
    "from checkpointer import Checkpointer\n",
    "from scheduler import build_warmup_cosine_tokens\n",
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "import os\n",
    "import signal\n",
    "import time\n",
    "\n",
    "# Set up optimizer, learning-rate scheduler, and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "scheduler = build_warmup_cosine_tokens(optimizer, target_tokens, config.WARMUP_PCT)\n",
    "lossFn = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "checkpointer = Checkpointer(checkpoint_dir, model, optimizer, scheduler, device=device)\n",
    "\n",
    "# Load the latest checkpoint if available.\n",
    "resume_epoch, resume_step, global_step, resume_sample_index, resume_tokens, resume_state = checkpointer.load_latest()\n",
    "\n",
    "# Align the scheduler with the resumed token count.\n",
    "if resume_tokens:\n",
    "    scheduler.last_epoch = resume_tokens # we misuse token's epoch count for tokens\n",
    "    for group, base_lr, lr_lambda in zip(optimizer.param_groups, scheduler.base_lrs, scheduler.lr_lambdas):\n",
    "        group[\"lr\"] = base_lr * lr_lambda(resume_tokens)\n",
    "\n",
    "# Normalize resume state into per-spec row offsets.\n",
    "# Keep offsets from the checkpoint, even for specs not active in this run.\n",
    "# Ensure current specs always have a default offset for safe lookups.\n",
    "# This drives shard/row skipping during resume and checkpointing.\n",
    "resume_rows = normalize_resume_rows(resume_state, dataset_specs)\n",
    "source_row_counts = dict(resume_rows) # Track row offsets for shard-aware resume.\n",
    "\n",
    "# Report when resuming via the legacy sample index.\n",
    "use_row_resume = any(resume_rows.get(spec[\"spec\"], 0) > 0 for spec in dataset_specs)\n",
    "# Track the dataset position for skipping/checkpointing when row offsets are unavailable.\n",
    "loader_skip_samples = 0 if use_row_resume else resume_sample_index\n",
    "if resume_sample_index > 0 and not use_row_resume:\n",
    "    print(\n",
    "        f\"Resume rows unavailable; falling back to linear sample skip ({resume_sample_index}).\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "# Build packed datasets per source with row-offset resumes.\n",
    "# Resolve shard-aware resume plans, then stream from the right shard/offset.\n",
    "# Pack each source into fixed-length token blocks with source IDs.\n",
    "# Interleave happens later, so each dataset is prepared independently.\n",
    "# Row offsets are tracked for checkpoint-safe restarts.\n",
    "packed_datasets = []\n",
    "for dataset_index, spec in enumerate(dataset_specs):\n",
    "    row_offset = resume_rows.get(spec[\"spec\"], 0)\n",
    "    data_files, in_shard_offset, shard_label = resolve_resume_plan(\n",
    "        spec,\n",
    "        row_offset,\n",
    "        cache_dir=data_dir,\n",
    "    )\n",
    "    raw_streaming = load_dataset_from_spec(\n",
    "        spec,\n",
    "        cache_dir=data_dir,\n",
    "        streaming=True,\n",
    "        data_files=data_files,\n",
    "    )\n",
    "    if row_offset > 0:\n",
    "        if data_files is None:\n",
    "            print(\n",
    "                f\"Resume rows (linear): {spec['spec']} -> {row_offset}\",\n",
    "                flush=True,\n",
    "            )\n",
    "            raw_streaming = raw_streaming.skip(row_offset)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Resume rows: {spec['spec']} -> {shard_label} +{in_shard_offset}\",\n",
    "                flush=True,\n",
    "            )\n",
    "            raw_streaming = raw_streaming.skip(in_shard_offset)\n",
    "    packed = build_packed_dataset(\n",
    "        raw_streaming,\n",
    "        tokenizer=tokenizer,\n",
    "        block_size=config.CONTEXT_LEN,\n",
    "        text_key=spec[\"text_key\"],\n",
    "        pack_batch_size=config.PACK_BATCH_SIZE,\n",
    "        source_id=dataset_index,\n",
    "    )\n",
    "    packed_datasets.append(packed)\n",
    "\n",
    "base_dataset = build_interleaved_dataset(packed_datasets, seed=42)\n",
    "print(f\"Packed dataset ready ({len(packed_datasets)} sources).\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4c1b059e553ea",
   "metadata": {},
   "source": [
    "## Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772b7c40f5ca098",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-21T08:00:25.323665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from /Users/sts/Quellen/nanoschnack/checkpoints/latest.pt at epoch 0, step 98.\n",
      "Epoch 1 (Step 100, Global 100), Loss: 6.3413, Samples/s: 5.2\n"
     ]
    }
   ],
   "source": [
    "last_ckpt_time = time.time()\n",
    "\n",
    "# Track current counters for checkpointing and interrupts.\n",
    "current_epoch = resume_epoch\n",
    "current_step = resume_step\n",
    "current_sample_index = resume_sample_index\n",
    "\n",
    "# Initialize the progress logger to display training progress and loss\n",
    "progress = ProgressLogger(\n",
    "    plot_with_completion,\n",
    "    start_global_step=global_step,\n",
    "    start_total_samples=resume_sample_index,\n",
    "    start_total_tokens=resume_tokens,\n",
    "    log_interval=config.LOG_INTERVAL_SECS,\n",
    "    warmup_plot_interval=config.PLOT_WARMUP_SECS,\n",
    "    plot_interval=config.PLOT_INTERVAL_SECS,\n",
    "    warmup_window_secs=config.WARMUP_WINDOW_SECS,\n",
    "    estimated_total_tokens=target_tokens,\n",
    ")\n",
    "\n",
    "# Enable debug output with DEBUG levels.\n",
    "debug_level = int(os.getenv(\"DEBUG\", \"0\"))\n",
    "printed_debug_sample = False\n",
    "\n",
    "# Track SIGINT so we can checkpoint after a safe step.\n",
    "stop_requested = {\"flag\": False}\n",
    "def _request_stop(signum, frame):\n",
    "    # Record interrupt without raising inside the signal handler.\n",
    "    print(\"Interrupted: saving checkpoint...\")\n",
    "    stop_requested[\"flag\"] = True\n",
    "signal.signal(signal.SIGINT, _request_stop)\n",
    "print(\"Starting training loop...\", flush=True)\n",
    "for current_epoch in itertools.count(resume_epoch):\n",
    "    # Reset row counters at epoch boundaries beyond the resume epoch.\n",
    "    if current_epoch != resume_epoch:\n",
    "        for spec in dataset_specs:\n",
    "            source_row_counts[spec[\"spec\"]] = 0\n",
    "    dataset_epoch = base_dataset.shuffle(buffer_size=config.SHUFFLE_BUFFER, seed=42 + current_epoch)\n",
    "    dataset_epoch = dataset_epoch.with_format(\"torch\")\n",
    "    if current_epoch == resume_epoch and loader_skip_samples > 0:\n",
    "        dataset_epoch = dataset_epoch.skip(loader_skip_samples)\n",
    "    loader = DataLoader(dataset_epoch, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    for current_step, batch in enumerate(loader):\n",
    "        # Move batch tensors to the device and prepare an optional attention mask.\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        attn_mask = None\n",
    "        if attention_mask is not None and not attention_mask.all():\n",
    "            attn_mask = attention_mask\n",
    "\n",
    "        # Build next-token prediction pairs.\n",
    "        inputs = input_ids[:, :-1] # everything from the first token except the last\n",
    "        targets = input_ids[:, 1:] # everything from the second token onward\n",
    "\n",
    "        # Preview tokenization outputs for debugging.\n",
    "        if debug_level >= 2 and not printed_debug_sample:\n",
    "            input_preview = inputs[0].tolist()\n",
    "            target_preview = targets[0].tolist()\n",
    "            print(f\"Input tokens: {input_preview}\")\n",
    "            print(f\"Target tokens: {target_preview}\")\n",
    "            print(f\"Input text: {tokenizer.decode(input_preview)}\")\n",
    "            print(f\"Target text: {tokenizer.decode(target_preview)}\")\n",
    "            printed_debug_sample = True\n",
    "\n",
    "        # Dump decoded inputs for every sample in the batch.\n",
    "        if debug_level >= 6:\n",
    "            for debug_sample_index, sample_ids in enumerate(input_ids.tolist()):\n",
    "                decoded = tokenizer.decode(sample_ids)\n",
    "                print(f\"Encoded input {debug_sample_index}: {decoded}\")\n",
    "\n",
    "        # Clear accumulated gradients before the forward/backward pass.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Run the forward pass with autocast and compute loss.\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16) if device.type == \"cuda\" else contextlib.nullcontext():\n",
    "            logits = model(inputs, attention_mask=attn_mask[:, :-1] if attn_mask is not None else None)\n",
    "            loss = lossFn(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "\n",
    "        # Backpropagate and apply gradient clipping.\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Apply the optimizer step and advance the token scheduler.\n",
    "        optimizer.step()\n",
    "        token_count = attention_mask[:, 1:].sum().item()\n",
    "        next_total_tokens = progress.total_tokens + token_count\n",
    "        scheduler.last_epoch = next_total_tokens - 1\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log progress and plot loss history.\n",
    "        progress.tick(\n",
    "            loss.item(),\n",
    "            input_ids.size(0),\n",
    "            token_count,\n",
    "            optimizer.param_groups[0][\"lr\"],\n",
    "            current_epoch,\n",
    "            current_step,\n",
    "            remaining_tokens=max(target_tokens - next_total_tokens, 0),\n",
    "        )\n",
    "\n",
    "        # Advance per-source row counters for resume safety.\n",
    "        row_counts = batch[\"row_count\"].tolist()\n",
    "        source_ids = batch[\"source_id\"].tolist()\n",
    "        for source_id, row_count in zip(source_ids, row_counts):\n",
    "            if row_count:\n",
    "                spec_key = dataset_specs[int(source_id)][\"spec\"]\n",
    "                source_row_counts[spec_key] += int(row_count)\n",
    "\n",
    "        # Update checkpoint counters and save when needed.\n",
    "        current_sample_index += input_ids.size(0)\n",
    "        now = time.time()\n",
    "        ckpt_interval = config.CHECKPOINT_WARMUP_SECS if (now - last_ckpt_time) < config.WARMUP_WINDOW_SECS else config.CHECKPOINT_INTERVAL_SECS\n",
    "        should_checkpoint = (now - last_ckpt_time >= ckpt_interval) or stop_requested[\"flag\"]\n",
    "        if should_checkpoint:\n",
    "            checkpointer.save_latest(\n",
    "                current_epoch,\n",
    "                current_step,\n",
    "                progress.global_step,\n",
    "                current_sample_index,\n",
    "                progress.total_tokens,\n",
    "                resume_state=build_resume_state(source_row_counts, dataset_specs),\n",
    "            )\n",
    "            last_ckpt_time = now\n",
    "\n",
    "        # Exit after the current step if SIGINT was requested.\n",
    "        if stop_requested[\"flag\"]:\n",
    "            break\n",
    "    loader_skip_samples = 0\n",
    "    current_sample_index = 0\n",
    "    if stop_requested[\"flag\"]:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
