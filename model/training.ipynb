{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce047449aa6ea1b",
   "metadata": {},
   "source": [
    "# NanoSchnack Model\n",
    "\n",
    "## Setup\n",
    "\n",
    "- Install dependencies.\n",
    "- Verify that MPS is available (for Apple Silicon GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe08928ac36354a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:00:23.419351Z",
     "start_time": "2025-12-21T08:00:22.409179Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from device import device_info, pick_device, print_device_info\n",
    "\n",
    "device = pick_device()\n",
    "info = device_info(device)\n",
    "print_device_info(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f746ce3bad99a",
   "metadata": {},
   "source": [
    "## Loading a tokenizer with Hugging Face's tokenizer library\n",
    "\n",
    "- Compare: https://github.com/huggingface/tokenizers\n",
    "- Tiktokenizer: https://tiktokenizer.vercel.app/?model=gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db31e04be138c071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:00:23.702221Z",
     "start_time": "2025-12-21T08:00:23.423961Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sts/src/nanoschnack/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import load_tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dbfcc99775200",
   "metadata": {},
   "source": [
    "## Instantiating the NanoSchnack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71dce028f07aaaf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:00:24.153419Z",
     "start_time": "2025-12-21T08:00:23.767280Z"
    }
   },
   "outputs": [],
   "source": [
    "from gpt import GPT\n",
    "from autotune import find_max_batch_size\n",
    "from config import (\n",
    "    BATCH_SIZE,\n",
    "    CHECKPOINT_INTERVAL_SECS,\n",
    "    CHECKPOINT_WARMUP_SECS,\n",
    "    CONTEXT_LEN,\n",
    "    EMBED_SIZE,\n",
    "    HIDDEN_SIZE,\n",
    "    LEARNING_RATE,\n",
    "    LOG_INTERVAL_SECS,\n",
    "    MAX_NEW_TOKENS,\n",
    "    NUM_HEADS,\n",
    "    NUM_LAYERS,\n",
    "    PLOT_INTERVAL_SECS,\n",
    "    PLOT_WARMUP_SECS,\n",
    "    TEMPERATURE,\n",
    "    TOP_K,\n",
    "    WARMUP_WINDOW_SECS,\n",
    "    print_training_hyperparams,\n",
    ")\n",
    "\n",
    "# add special tokens\n",
    "tokenizer.add_special_tokens([\"[PAD]\"])\n",
    "pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "\n",
    "context_len = CONTEXT_LEN\n",
    "model = GPT(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    context_len=CONTEXT_LEN,\n",
    ").to(device).train()\n",
    "print_training_hyperparams(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dffa18e4d2fb4",
   "metadata": {},
   "source": [
    "## Load the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea7406035cdae00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:00:25.276715Z",
     "start_time": "2025-12-21T08:00:24.153843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Resolve model paths so relative data/checkpoint locations are stable.\n",
    "try:\n",
    "    from model import setup_paths\n",
    "except ModuleNotFoundError:\n",
    "    from __init__ import setup_paths\n",
    "model_dir, data_dir, checkpoint_dir = setup_paths()\n",
    "\n",
    "from datasets.utils.logging import enable_progress_bar, set_verbosity_warning\n",
    "from loader import ShardedBatchLoader\n",
    "\n",
    "# Download shards on demand and shuffle within each shard.\n",
    "set_verbosity_warning()\n",
    "enable_progress_bar()\n",
    "\n",
    "# do or not do chunking of the input text, instead of truncating.\n",
    "if False:\n",
    "    max_len = context_len\n",
    "    stride = context_len//4  # overlap; set to 0 for no overlap\n",
    "\n",
    "    tokenizer.disable_truncation()\n",
    "    tokenizer.disable_padding()\n",
    "\n",
    "    # Split long sequences into fixed windows, optionally with overlap.\n",
    "    def chunk_ids(ids, max_len, stride):\n",
    "        if len(ids) == 0:\n",
    "            return []\n",
    "        step = max_len - stride\n",
    "        chunks = []\n",
    "        for start in range(0, len(ids), step):\n",
    "            chunk = ids[start:start + max_len]\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            if len(chunk) < max_len:\n",
    "                chunk = chunk + [pad_id] * (max_len - len(chunk))\n",
    "            chunks.append(chunk)\n",
    "            if start + max_len >= len(ids):\n",
    "                break\n",
    "        return chunks\n",
    "\n",
    "    def tokenizer_batch(batch):\n",
    "        input_ids = []\n",
    "        attention_mask = [] # marks real tokens (1) vs padding (0)\n",
    "        for text in batch[\"result\"]:\n",
    "            ids = tokenizer.encode(text).ids\n",
    "            for chunk in chunk_ids(ids, max_len=max_len,\n",
    "                                   stride=stride):\n",
    "                input_ids.append(chunk)\n",
    "                attention_mask.append([1 if t != pad_id else 0 for t\n",
    "                                       in chunk])\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "else:\n",
    "    # Enable truncation and padding\n",
    "    tokenizer.enable_truncation(max_length=context_len)\n",
    "    tokenizer.enable_padding(length=context_len, pad_id=pad_id, pad_token=\"[PAD]\")\n",
    "\n",
    "    # Wrap Hugging Face tokenizer for batch processing\n",
    "    def tokenizer_batch(batch):\n",
    "        token_batch = tokenizer.encode_batch(batch[\"result\"])\n",
    "        return {\n",
    "            \"input_ids\": [e.ids for e in token_batch],\n",
    "            \"attention_mask\": [e.attention_mask for e in token_batch], # marks real tokens (1) vs padding (0)\n",
    "        }\n",
    "\n",
    "# Tokenize the dataset\n",
    "tuned_batch_size = find_max_batch_size(\n",
    "    model,\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    seq_len=context_len,\n",
    "    device=device,\n",
    "    start=BATCH_SIZE,\n",
    ")\n",
    "batch_size = tuned_batch_size or BATCH_SIZE\n",
    "print(f\"Tuned batch_size={batch_size}\")\n",
    "sharded_loader = ShardedBatchLoader(\n",
    "    repo_id=\"pdelobelle/fineweb-german-edu-mt\",\n",
    "    data_dir=data_dir,\n",
    "    tokenizer_batch=tokenizer_batch,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    ")\n",
    "print(f\"Sharded loader ready ({sharded_loader.num_shards} shards).\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484ece969be72dd",
   "metadata": {},
   "source": [
    "## Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416b74888ed605b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-21T08:00:25.323665Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from /Users/sts/Quellen/nanoschnack/checkpoints/latest.pt at epoch 0, step 98.\n",
      "Epoch 1 (Step 100, Global 100), Loss: 6.3413, Samples/s: 5.2\n"
     ]
    }
   ],
   "source": [
    "from plot import ascii_loss_plot\n",
    "from progress import ProgressLogger\n",
    "from checkpointer import Checkpointer\n",
    "import math\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Set up optimizer, learning-rate scheduler, and loss function\n",
    "epochs = 1 # epochs between 1 and 3 are usually sufficient for good results, rather 1 than 3.\n",
    "estimated_total_samples = sharded_loader.estimate_total_samples()\n",
    "steps_per_epoch = math.ceil(estimated_total_samples / batch_size)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "print(f\"Estimated steps per epoch: {steps_per_epoch} (total {total_steps}).\", flush=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "lossFn = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "# The checkpointer will save and load model/optimizer/scheduler states to/from disk.\n",
    "checkpointer = Checkpointer(checkpoint_dir, model, optimizer, scheduler, device=device)\n",
    "resume_epoch, resume_step, global_step, resume_position, total_samples = checkpointer.load_latest()\n",
    "\n",
    "last_ckpt_time = time.time()\n",
    "\n",
    "# Initialize the progress logger to display training progress and loss\n",
    "progress = ProgressLogger(\n",
    "    ascii_loss_plot,\n",
    "    start_global_step=global_step,\n",
    "    start_total_samples=total_samples,\n",
    "    log_interval=LOG_INTERVAL_SECS,\n",
    "    warmup_plot_interval=PLOT_WARMUP_SECS,\n",
    "    plot_interval=PLOT_INTERVAL_SECS,\n",
    "    warmup_window_secs=WARMUP_WINDOW_SECS,\n",
    ")\n",
    "\n",
    "last_epoch = resume_epoch\n",
    "last_step = resume_step\n",
    "current_position = resume_position\n",
    "total_samples = total_samples\n",
    "try:\n",
    "    print(\"Starting training loop...\", flush=True)\n",
    "    for epoch in range(resume_epoch, epochs):\n",
    "        last_epoch = epoch\n",
    "        loader = sharded_loader.iter_batches(start_position=current_position)\n",
    "        for step, (batch, current_position, shard_index, shard_len) in enumerate(loader):\n",
    "            last_step = step\n",
    "\n",
    "            # Get the input IDs and attention mask, and move them to the GPU\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Next-token prediction\n",
    "            inputs = input_ids[:, :-1] # everything from the first token except the last\n",
    "            targets = input_ids[:, 1:] # everything from the second token onward\n",
    "\n",
    "            # Clear accumulated gradients from the previous step (which torch does automatically otherwise)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(inputs, attention_mask=attention_mask[:, :-1])\n",
    "\n",
    "            # Compute (average) loss of the predicted next tokens and apply backpropagation.\n",
    "            # reshape to (batch_size * seq_len, vocab_size) and (batch_size * seq_len)\n",
    "            loss = lossFn(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights, then advance the learning-rate schedule.\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Log progress and plot loss history\n",
    "            progress.tick(\n",
    "                loss.item(),\n",
    "                input_ids.size(0),\n",
    "                epoch,\n",
    "                step,\n",
    "                shard_index=shard_index,\n",
    "                shard_count=sharded_loader.num_shards,\n",
    "                shard_len=shard_len,\n",
    "            )\n",
    "            total_samples += input_ids.size(0)\n",
    "            now = time.time()\n",
    "            ckpt_interval = CHECKPOINT_WARMUP_SECS if (now - last_ckpt_time) < WARMUP_WINDOW_SECS else CHECKPOINT_INTERVAL_SECS\n",
    "            if now - last_ckpt_time >= ckpt_interval:\n",
    "                checkpointer.save_latest(\n",
    "                    epoch,\n",
    "                    step,\n",
    "                    progress.global_step,\n",
    "                    current_position,\n",
    "                    total_samples,\n",
    "                )\n",
    "                last_ckpt_time = now\n",
    "        current_position = (0, 0)\n",
    "except KeyboardInterrupt:\n",
    "    # Save a checkpoint so training can resume from the last completed step.\n",
    "    print(\"Interrupted: saving checkpoint...\")\n",
    "    checkpointer.save_latest(\n",
    "        last_epoch,\n",
    "        last_step,\n",
    "        progress.global_step,\n",
    "        current_position,\n",
    "        total_samples,\n",
    "    )\n",
    "    print(\"Interrupted: checkpoint saved, exiting.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
